---
title: "Individual Assignment 1"
author: "Jan Hynek"
date: "19 listopadu 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Loading Libraries
```{r libraries, warning=FALSE,  message=FALSE}
library(tidyverse)
library(reshape2)
library(purrr)
library(viridis)
library(ggthemes)
library(assertthat)
library(plotly)
library(ggthemes)


font.add.google("Lato", "lato")
font.add.google("Raleway", "raleway")

set.seed(921021)
```


## Exercise 1 - Transforming X using 9 Gaussian basis functions.
_Transform the $x$ by using 9 Gaussian basis functions. Fix the parameters of your gaussian functions. Take note of the domain of the function when you select the parameter governing the spatial scale (i.e. $s$). Share your code and graph the results. You need to use your result in the succeeding questions. (1 point)_

In this part I create functions, which are used throughout the script. They will be handy, as we are creating several different sizes of the dataset.

This function creates data of the specified length. Returns two vectors in a dataframe - x, and target.
```{r data_gen}
data_gen <- function(n){
  x <- runif(n, min = -1, max = 1)
  noise <- rnorm(n, mean = 0, sd = 0.1)
  target <- sin(2 * pi * x) + noise
  return(data.frame(x, target))
}
```


This is the gaussian basis function. Transforms the vector using gaussian function $\phi_j(x) = exp\left(\frac{(x-\mu_j)^2}{2s^2}\right) $.
```{r gauss}
gauss_basis <- function(x, mu, s_sq){
  return(
    exp(
      (-(x - mu) ^ 2) / (2 * s_sq)
    )
  )
}
```

This function takes dataset created from data_gen, and transforms it using 9 Gaussian basis functions. returns n times 11  matrix, where 1st columns is x, 2nd is target and next 9 are transformed gaussian bases. I can also modify $s^2$, however it is preset to 0.2. I will shortly present why.
```{r gauss_basis}
gauss_basis_gen <- function(data_frame, s_sq = 0.02) {
#  x <- data_frame$x
  result <- data_frame
  counter <- 1
  for (i in seq(-1, 1, 0.25)) {
    result <- result %>% 
      mutate(gauss_basis(x, mu = i, s_sq = s_sq))
    colnames(result)[dim(result)[2]] <- paste('gauss', counter, sep = "_")
    counter <- counter + 1
  }
  return(result)
}
```

this function  quickly creates test data. It will be handy throughout the analysis.
variables are stored in global env. And because it is otherwise annoying
in the editor, I also create them once here. But they will also be useful for creating graph.
```{r}

values_gb <- data_gen(100) %>% 
  gauss_basis_gen(s_sq = 0.02)
phi_mat <- values_gb %>% 
  select(starts_with("gauss")) %>% 
  as.matrix()
t_mat <- as.matrix(values_gb$target) 


create_test_data <- function(n = 100, s_sq = 0.1) {
  values_gb <<- data_gen(n) %>% 
    gauss_basis_gen(s_sq)
  phi_mat <<- values_gb %>% 
    select(starts_with("gauss")) %>% 
    as.matrix()
  t_mat <<- as.matrix(values_gb$target)  
}
```


```{r}
my_theme <- theme(plot.subtitle = element_text(vjust = 1), 
        plot.caption = element_text(vjust = 1), 
        panel.grid.major = element_line(colour = "gray94"), 
        panel.grid.minor = element_line(linetype = "blank"), 
        panel.background = element_rect(fill = NA)) 
```

Now, we generate data using presented functions and plot them.
```{r graph}

create_test_data(s_sq = 0.02)
values_gb %>% 
  melt(id.vars = c("x", "target"), variable.name = "gauss_fun") %>% 
  ggplot(aes(x = x, y = value)) + 
  geom_line(aes(colour = gauss_fun), size = 1) + 
  geom_point(aes(y = target, colour = "target"), alpha = 0.1) + 
  ggtitle("Gaussian bases and target data for s_sq = 0.02") +   
  theme(text = element_text(family = "raleway")) + my_theme
```
Selecting $s^2$ is tricky. It seems as a good option to fit $s^2 = 0.02$, as then four of the gaussian bases would fit nicely at the sinusoid.

We can also have a look at  $s^2 = 0.1$:
```{r}
create_test_data(s_sq = 0.1)
values_gb %>% 
  melt(id.vars = c("x", "target"), variable.name = "gauss_fun") %>% 
  ggplot(aes(x = x, y = value)) + 
  geom_line(aes(colour = gauss_fun), size = 1) + 
  geom_point(aes(y = target, colour = "target"), alpha = 0.1) + 
  theme_pander() + 
  scale_colour_pander() + ggtitle("Gaussian bases and target data for s_sq = 0.1")
```



I will also plot gaussian bases for $s^2 = 0.2$. These values of $s^2$ will be also discussed later throughout the assignment.


```{r}
create_test_data(s_sq = 0.2)
values_gb %>% 
  melt(id.vars = c("x", "target"), variable.name = "gauss_fun") %>% 
  ggplot(aes(x = x, y = value)) + 
  geom_line(aes(colour = gauss_fun), size = 1) + 
  geom_point(aes(y = target, colour = "target"), alpha = 0.1) + 
  theme_pander() + 
  scale_colour_pander() + ggtitle("Gaussian bases and target data for s_sq = 0.2")
```



## Exercise 2 - Regularized Least squares
_Estimate the parameters using regularized least squares for sample sizes: 20,40,…,100. provide an overview table with point estimates for the parameters. (2 points)_

In this section we first create function, which directly calculates RLS. This is done analytically.
Function also checks, whether input is matrix.
```{r regularized LS}
RLS <- function(phi_mat, target, lambda) {
  assert_that(is.matrix(phi_mat))
  assert_that(is.matrix(target))
  
  w_RLS <- (solve(lambda * diag(dim(phi_mat)[2]) + t(phi_mat) %*% phi_mat) %*% t(phi_mat)) %*% target
  return(w_RLS)
}

```



Now we run for loop, each for different amount of data. There is trick used that I multiply i by twenty, and therefore create different amount of data using data_gen function. I also decided to use $\lambda = 1$. This is chosen arbitrarily, but I could use cross validation to evaluate the optimal $\lambda$, among other methods. However, this is not in the scope of this assignment.

```{r}

w_RLS_result <- matrix(ncol = 5, nrow = 9)
colnames(w_RLS_result) <- c(paste("n", (1:5) * 20, sep = "_"))

for (i in 1:5) {
  create_test_data(n = (20 * i), s_sq = 0.1)
  w_RLS_result[, i] <- RLS(phi_mat = phi_mat, 
                                   target = t_mat, 
                                   lambda = 0.2)
}


w_RLS_result
```


## Exercise 3

_Estimate the parameters using bayesian linear regression assuming that the parameters have multivariate normal prior: 20, 40,…,100. Here, you can preselect the $\beta$ and $\alpha$. Provide an overview table with point estimates for the parameters. (2 points)_

In this part I use two different methods. I use Bayesian Linear Regression with and without updating. I chose $s^2$ equal to 0.1, and decided to use $\alpha = 1$ and $\beta = 25$. These alpha represents my prior beliefs about precision, beta represents the "weight" for prior and posterior distributions.
```{r BLR}
BLR <- function(phi_mat, target, alpha, beta) {
  assert_that(is.matrix(phi_mat))
  assert_that(is.matrix(target))
  
  m_0 <- rep(0, 9)
  S_0 <- alpha * diag(1, 9)
  
  
  S_N <- solve(solve(S_0) + beta * t(phi_mat) %*% phi_mat)
  m_N <- S_N %*% (solve(S_0) %*% m_0 + beta * t(phi_mat) %*% target)
  
  
  return(m_N)
}
```



```{r BLR_updating}
BLR_updating <- function(phi_mat, target, alpha, beta) {
  assert_that(is.matrix(phi_mat))
  assert_that(is.matrix(target))
  #setting prior
  m_0 <- as.matrix(rep(0, 9))
  S_0 <- alpha * diag(1, 9)
  # for proper working, m_N needs to be defined beforehand. Let's set it the same.
  m_N <- m_0
  S_N <- S_0
  
  
  for (i in 1:dim(phi_mat)[1]) {
    m_0 <- m_N # old posterior is new prior.
    S_0 <- S_N
    phi <- t(as.matrix(phi_mat[i, ]))
    S_N <- solve(solve(S_0) + (beta * t(phi) %*% phi))
    m_N <- S_N %*% (solve(S_0) %*% m_0 + (beta * t(phi) %*% target[i]))
  }
  
  return(m_N)
  
}
```



We can have a look at the results.

```{r BLR results, echo=FALSE}
w_BLR_result <- matrix(ncol = 5, nrow = 9)
colnames(w_BLR_result) <- c(paste("n", (1:5) * 20, sep = "_"))

w_BLRupd_result <- matrix(ncol = 5, nrow = 9)
colnames(w_BLRupd_result) <- c(paste("n", (1:5) * 20, sep = "_"))



for (i in 1:5) {
  create_test_data(n = (20 * i), s_sq = 0.1)
  
  w_BLR_result[, i] <- BLR(phi_mat = phi_mat, 
                                   target = t_mat, 
                                   alpha = 1, beta = 25)
  w_BLRupd_result[, i] <- BLR_updating(phi_mat = phi_mat, 
                                   target = t_mat, 
                                   alpha = 1, beta = 25)
}

w_BLR_result

```

```{r}
w_BLRupd_result
```
We can observe that both of these methods provide us with identical weights. Therefore I will use easier method without updating later on in the assignment.



## Exercise 4 - comparision
_Discuss how the parameter estimates for the two methods compare, you can use tables and graphs to support your claims. (2 points)_

I will use graphs to show how these methods compare.

```{r plot_interactive_graph_BLS_RLS, message=FALSE, warning=FALSE, echo=FALSE}
####################
# Exercise 4 - RLS  - graph#
####################

plot_interactive_graph_BLS_RLS <- function(s_squared, 
                                   lambda_RLS = 0.01, 
                                   alpha_BLR = 1, 
                                   beta_BLR = 25, n_obs = 101) {
  values <- data_gen(10)
  values_gaussbases <- values %>% 
    gauss_basis_gen(s_sq = s_squared) %>% 
    mutate(n = 10)
  
  for (i in 5:25) {
    val_temp <- data_gen(i * 4) %>% 
      gauss_basis_gen(s_sq = 0.1) %>% 
      mutate(n = 4 * i)
    values_gaussbases <- rbind(values_gaussbases, val_temp)
  }
  
  # finding coefficients and then predictions for every set of data.
  values_predictions <- values %>% 
    mutate(preds_BLR = 0)  %>%  
    mutate(preds_RLS = 0) %>%
    mutate(n = 0)
  
  
  
  values_preds_temp <- data_gen(n_obs)  %>% # OCD unfriendly
    gauss_basis_gen(s_sq = s_squared) %>% 
    mutate(n = 0)
  phi_mat_temp <- values_preds_temp %>% 
      select(starts_with("gauss")) %>% 
      as.matrix()
  
  for (i in unique(values_gaussbases$n)) {
    val_temp <- values_gaussbases %>% filter(n == i)
    
    phi_mat <- val_temp %>% 
      select(starts_with("gauss")) %>% 
      as.matrix()
    t_mat <- as.matrix(val_temp$target)
    
    coefs_RLS    <- RLS(phi_mat, t_mat, lambda = lambda_RLS)
    coefs_BLR    <- BLR(phi_mat, t_mat, alpha = alpha_BLR, beta = beta_BLR)
    
    val_temp_2 <- cbind(values_preds_temp$x, 
                        values_preds_temp$target, 
                        phi_mat_temp %*% as.matrix(coefs_BLR), 
                        phi_mat_temp %*% as.matrix(coefs_RLS),
                        rep(i, n_obs))
    
    # print(dim(val_temp_2))
    colnames(val_temp_2) <- names(values_predictions)
    values_predictions <- rbind(values_predictions, val_temp_2)
  }
  
  
  # 
  values_predictions <- values_predictions %>% 
     filter(n != 0)
  
  # plotting !
  p <- ggplot(values_predictions, aes(x = x))  +
    geom_line(aes(y = preds_BLR, frame = n, colour = sprintf("BLR, alpha = %s, beta = %s", alpha_BLR, beta_BLR))) +
    geom_line(aes(y = preds_RLS, frame = n, colour = sprintf("RLS, lambda = %s", lambda_RLS))) + 
    geom_point(aes(x = x, y = target, frame = n, colour = "target"), alpha = 0.3) + 
    geom_line(aes(x = x, y = sin(2 * pi * x), colour = "sin(2*pi*x)"), alpha = 0.3)  +
    ggtitle(sprintf("Comparision of performance of RLS and BLR when s_squared = %s", s_squared)) + 
    labs(y = "predictions and targets", colour = "colours") + my_theme
  plot <- ggplotly(p) %>%
    animation_opts(easing = "linear",redraw = FALSE)
  plot
}
```


In the following graph, we can control on how many points were the models trained and then we predict 100 points using these estimates. I set the values to the same values chosen in previous parts. I will also select $s^2 = 0.1$. We can observe throughout the assignment that this value provides us with best results.

Regarding the models, we can observe that in this setup BLR is performing better on small samples and quickly converges to true distribution . However $\lambda$ in RLS is restricting overfitting of the data. In this case it is too restrictive and therefore BLR is closer to the original distribution.

```{r plot BLS RLS 1}
plot_interactive_graph_BLS_RLS(s_squared = 0.1, lambda_RLS = 1, alpha_BLR = 1, beta_BLR = 25)

```


In the following graph I have set $s^2$ to smaller value, and at this moment BLR is prone to overfitting of the values and even though it is better performing with small amount of observations, for higher amount of observations it is outperformed by strongly restricted RLS, which better 

```{r BLS RLS 2, warning=FALSE, error=FALSE, message}
plot_interactive_graph_BLS_RLS(s_squared = 0.05, lambda_RLS = 1, alpha_BLR = 1, beta_BLR = 25)
```

In the next graph we can observe that if we choose too big variance of the bases, even unrestricted LS and optimal BLR is useless. Therefore, we should be careful with selecting parameters of gaussian basis functions.

```{r BLS RLS 3}
plot_interactive_graph_BLS_RLS(s_squared = 0.2, lambda_RLS = 0, alpha_BLR = 2, beta_BLR = 100)
```

## Exercise 5 - estimating $\alpha$ and $\beta$ of BLR
In this section I use simplified version of BLR based on equations 3.53 and 3.54 of Bishop (2009). And using 3.90, 3.91, 3.92 and 3.95 I estimate hyperparameters $\alpha$ and $\beta$, which are used afterwards for estimating parameters of Gauss basis functions.


```{r BLR_optimal, echo=FALSE}
#########
# Ex 5  #
#########

BLR_iter <- function(phi_mat, target, alpha, beta){
  assert_that(is.matrix(phi_mat))
  assert_that(is.matrix(target))
  
  S_N <- solve(alpha * diag(1, 9) + beta * t(phi_mat) %*% phi_mat)
  m_N <- beta * S_N %*%  t(phi_mat) %*% target
  
  
  return(list(m_N, S_N))
}

BLR_optimal <- function(phi_mat, t_mat, 
                        max_n_iter = 1000, 
                        min_d_alpha = 0.001, 
                        min_d_beta = 0.001, 
                        echo = FALSE) {
  alpha <- 0.5
  beta <- 5
  iteration <- 1 
  d_alpha <- 10
  d_beta <- 10
  
  while ((iteration < max_n_iter) & 
         ((abs(d_alpha) > min_d_alpha) | (abs(d_beta) > min_d_beta))) {
    lambdas <- eigen(beta * t(phi_mat) %*% phi_mat)$values
    gamma <- 0
    for (lambda in lambdas) {
      gamma <- gamma + (lambda/(alpha + lambda))
    }
    
    iter_mN_SN <- BLR_iter(phi_mat = phi_mat, 
                           target = t_mat, 
                           alpha = alpha, 
                           beta = beta)
    m_N <- iter_mN_SN[[1]]
    S_N <- iter_mN_SN[[2]]
    SSR <- 0
    for (i in 1:nrow(phi_mat)) {
      SSR <- SSR + (t_mat[i, ] - (t(m_N) %*% phi_mat[i, ]))^2
    }
    
    d_alpha <- (gamma / (t(m_N) %*% m_N)) - alpha
    alpha <- as.numeric(alpha + d_alpha)
    
    d_beta <- (1/((1 / (nrow(phi_mat) - gamma)) * SSR)) - beta
    beta <- as.numeric(beta + d_beta)
    iteration <- iteration + 1
  }
  if (echo) {
    print(paste("iteration", iteration))
    #print(as.numeric(d_alpha))
    #print(as.numeric(d_beta))
    print(paste("alpha", alpha))
    print(paste("beta", beta))
  }
  return(m_N)
}
```

Results, using $s^2 = 0.1$:

```{r, echo=FALSE}
w_BLRopt_result <- matrix(ncol = 5, nrow = 9)
colnames(w_BLRopt_result) <- c(paste("n", (1:5) * 20, sep = "_"))


for (i in 1:5) {
  create_test_data(n = (20 * i), s_sq = 0.1)
  print(paste("n", i * 20))
  w_BLRopt_result[, i] <- BLR_optimal(phi_mat, t_mat, echo = TRUE)
}
```

```{r}
w_BLRopt_result
```

It will be also interesting to see what is the performance of this model which uses optimal BLR.

```{r plot_interactive_graph, echo=FALSE}
plot_interactive_graph <- function(s_squared, 
                                   lambda_RLS = 0.01, 
                                   alpha_BLR = 0.5, 
                                   beta_BLR = 20) {
  values <- data_gen(10)
  values_gaussbases <- values %>% 
    gauss_basis_gen(s_sq = s_squared) %>% 
    mutate(n = 10)
  
  for (i in 5:25) {
    val_temp <- data_gen(i * 4) %>% 
      gauss_basis_gen(s_sq = 0.1) %>% 
      mutate(n = 4 * i)
    values_gaussbases <- rbind(values_gaussbases, val_temp)
  }
  
  # finding coefficients and then predictions for every set of data.
  values_predictions <- values %>% 
    mutate(preds_BLRopt = 0) %>%  
    mutate(preds_BLR = 0) %>%  
    mutate(preds_RLS = 0) %>%
    mutate(n = 0)
  
  
  
  values_preds_temp <- data_gen(101)  %>% # OCD unfriendly
    gauss_basis_gen(s_sq = s_squared) %>% 
    mutate(n = 0)
  phi_mat_temp <- values_preds_temp %>% 
      select(starts_with("gauss")) %>% 
      as.matrix()
  
  for (i in unique(values_gaussbases$n)) {
    val_temp <- values_gaussbases %>% filter(n == i) 
    
    phi_mat <- val_temp %>% 
      select(starts_with("gauss")) %>% 
      as.matrix()
    t_mat <- as.matrix(val_temp$target)
    
    coefs_BLRopt <- BLR_optimal(phi_mat, t_mat)
    coefs_RLS    <- RLS(phi_mat, t_mat, lambda = lambda_RLS)
    coefs_BLR    <- BLR(phi_mat, t_mat, alpha = alpha_BLR, beta = beta_BLR)
    
    val_temp_2 <- cbind(values_preds_temp$x, 
                        values_preds_temp$target, 
                        phi_mat_temp %*% as.matrix(coefs_BLRopt), 
                        phi_mat_temp %*% as.matrix(coefs_BLR), 
                        phi_mat_temp %*% as.matrix(coefs_RLS),
                        rep(i, 101))
    
    # print(dim(val_temp_2))
    colnames(val_temp_2) <- names(values_predictions)
    values_predictions <- rbind(values_predictions, val_temp_2)
  }
  
  
  # 
  values_predictions <- values_predictions %>% 
     filter(n != 0)
  
  # plotting !
  p <- ggplot(values_predictions, aes(x = x)) +
    geom_line(aes(y = preds_BLRopt, frame = n, colour = "optimal_BLR")) +
    geom_line(aes(y = preds_BLR, frame = n, colour = "BLR")) +
    geom_line(aes(y = (preds_RLS), frame = n, colour = "RLS")) + 
    geom_point(aes(x = x, y = target, frame = n, colour = "target"), alpha = 0.2) + 
    geom_line(aes(x = x, y = sin(2 * pi * x), colour = "sin(2*pi*x)"), alpha = 0.4) + 
    my_theme
  plot_s_sq <- ggplotly(p) %>%
    animation_opts(easing = "linear",redraw = FALSE)
  plot_s_sq
}



plot_interactive_graph(s_squared = 0.08, lambda_RLS = 0.01, alpha_BLR = 1, beta_BLR = 25)
```

```{r}
plot_interactive_graph(0.1)
```
```{r}
plot_interactive_graph(0.2)
```

